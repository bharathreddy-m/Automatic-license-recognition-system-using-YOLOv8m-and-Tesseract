# -*- coding: utf-8 -*-
"""License_plate_detector_+_OCR_+_Alert_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TagZ4XdI09EzKsaiGHnSMNOajeUNmFet
"""

# import all required libraries
! pip install ultralytics

# Unzip the file
import zipfile
import os

#path to the uploaded zip file
zip_file_path = "/content/License-Plate-Recognition.v2-2024-09-05-1-06am.yolov8.zip"

# unzip the file to /content folder
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
  zip_ref.extractall("/content/License-plate-dataset")

# Check the files extracted
extracted_files = os.listdir("/content/License-plate-dataset")
print(extracted_files)

# check the train, test and validation dataset size
train_dir = "/content/License-plate-dataset/train/images"
val_dir = "/content/License-plate-dataset/valid/images"
test_dir = "/content/License-plate-dataset/test/images"

# Get the number of images in each directory
train_size = len(os.listdir(train_dir))
val_size = len(os.listdir(val_dir))
test_size = len(os.listdir(test_dir))

print(f"Train set size - {train_size}")
print(f"Validation set size - {val_size}")
print(f"Test set size - {test_size}")

# Load YAML dataset
import yaml

#path to the data.yaml file
yaml_file_path = "/content/License-plate-dataset/data.yaml"

# Open and Load the YAML file
with open(yaml_file_path, 'r') as file:
  data_yaml = yaml.safe_load(file)

# Print the content of the YAML file
print(data_yaml)

# Extract class names from the YAML file
class_names = data_yaml.get('names', [])

# Print the list of classes
print("Classes in the Dataset:")
for class_id, class_name in enumerate(class_names):
    print(f"{class_id}: {class_name}")

import shutil
import os
from ultralytics import YOLO

# Set up Google Drive path
drive_path = '/content/drive/My Drive/Yolo_Weights/'

# Ensure the directory exists in Google Drive
if not os.path.exists(drive_path):
    os.makedirs(drive_path)

# Function to copy weights to Google Drive after each epoch
def save_weights_to_drive(epoch):
    # Define the path to the current weights
    weights_path = f'runs/detect/yolov8_license_plate/weights/epoch_{epoch}.pt'

    # Check if weights file exists
    if os.path.exists(weights_path):
        # Copy the weights file to Google Drive
        shutil.copy(weights_path, os.path.join(drive_path, f'epoch_{epoch}.pt'))
        print(f"Epoch {epoch} weights saved to Google Drive.")

# Initialize the YOLO model
model = YOLO("yolov8n.pt")

# Training loop with automatic save and callback
# Train the model on your dataset
model.train(
    data="/content/License-plate-dataset/data.yaml", # path to yaml file
    epochs = 10, # number of training epochs
    imgsz = 416, # input image size
    batch = 16, # adjust batch size depending on your RAM/GPU
    name = "yolov8_license_plate", # experiment name
    project="/content/drive/MyDrive/yolo_training",  # Saves everything here
    pretrained=True,
    patience = 5, # early stopping
    val = True, # Optional, explicitly states you want to validate each epoch
    save=True, # Automatically saves the work after each epoch
    half=True, # enable mixed precision training (FP16)
    workers=4, # number of workers for faster data loading
    # pin_memory=True # pin memory for faster data transfer to GPU (if needed)
)

# Ensure that weights are copied after each epoch automatically
for epoch in range(1, 11):  # Assuming training for 10 epochs
    save_weights_to_drive(epoch)  # This will copy weights to Google Drive after each epoch.

!ls /content/drive/MyDrive/yolo_training/yolov8_license_plate/weights/

from google.colab import files

# Download the best weights
files.download('/content/drive/MyDrive/yolo_training/yolov8_license_plate/weights/best.pt')

# check the best weights
from ultralytics import YOLO

# Load the model
model = YOLO("/content/yolov8n-10best.pt")

# Print model info
model.info()

from google.colab import drive
drive.mount('/content/drive')

import os

# Set a unique directory path for your current training session
save_dir = "/content/drive/MyDrive/yolo_training/yolov8m_license_plate_40epoch"
os.makedirs(save_dir, exist_ok=True)

# Train the model
from ultralytics import YOLO

# Load the best weights (ensure the path is correct)
model = YOLO("/content/yolov8n-10best.pt")

# Set your training parameters including image size, batch size, half precision, and workers
model.train(
    model='yolov8m.pt',                  # Use yolov8m architecture
    data="/content/License-plate-dataset/data.yaml",  # Path to your dataset config file
    epochs=40,                         # Train for 40 more epochs
    batch=16,                           # Batch size
    imgsz=416,                          # Image size (you can adjust this based on GPU capabilities)
    half=True,                          # Use half precision for faster training (requires CUDA)
    workers=4,                          # Number of data loading workers (adjust based on your environment)
    save=True,                          # Save weights after each epoch
    save_period=1,                      # Save every epoch
    project=save_dir,  # Path to save weights
    name='exp_yolov8m',                 # Name of experiment
    #resume=True,                        # Resume from previous model weights
)



# Test the model on test data
from ultralytics import YOLO
import os

# Path to the best weights file (adjust path accordingly)
weights_path = '/content/yolov8m-40best.pt'

# Load the trained YOLOv8 model with best weights
model = YOLO(weights_path)

#See model info
print(model.info())

# Path to your test dataset configuration file (adjust path accordingly)
test_dataset_path = '/content/License-plate-dataset/test/images'

# Run inference on the test set
results = model.predict(test_dataset_path)

# Show results (this will display the predictions on the test images)
results.show()

# Optionally, print detailed results (like mAP, precision, etc.)
print(results.pandas().xywh)  # Outputs predictions in a pandas DataFrame (xywh format)

import cv2
import numpy as np
import os
from matplotlib import pyplot as plt

# Path to your test images
test_images_path = '/content/License-plate-dataset/test/images/'

# Perform prediction on test images (for the first 200 images)
for i, img_path in enumerate(sorted(os.listdir(test_images_path))[:200]):
    img = cv2.imread(os.path.join(test_images_path, img_path))  # Read image
    results = model(img)  # Perform prediction

    # Draw bounding boxes on the image (if there are predictions)
    if results[0].boxes is not None and len(results[0].boxes.xyxy) > 0:  # Check if there are bounding boxes
        for box, conf, cls in zip(results[0].boxes.xyxy, results[0].boxes.conf, results[0].boxes.cls):  # Loop over predictions
            x1, y1, x2, y2 = map(int, box)  # Convert box coordinates to integers
            label = f"{model.names[int(cls)]} {conf:.2f}"  # Class label and confidence

            # Draw rectangle (bounding box)
            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)
            # Add label text
            cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)

    # Convert BGR image to RGB for displaying with Matplotlib
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Display the image with bounding boxes
    plt.imshow(img_rgb)
    plt.title(f"Prediction {i+1}")
    plt.axis('off')  # Hide axes
    plt.show()

    # Optional: You can break early to see only the first few images if needed
    if i == 100:  # Change this condition to adjust the number of images shown
        break

# see all results
validation_results = model.val()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir /path/to/log/folder



# accuracy of training set
from ultralytics import YOLO

# Load the trained model
best_model = YOLO('/content/yolov8m-40best.pt')

# Run validation-style evaluation on the TRAINING set
results = best_model.val(
    data='/content/License-plate-dataset/data.yaml',
    split='train',  # ‚Üê this is the secret sauce
    imgsz=416,
    conf=0.5
)

# Show metrics
print(results)

# accuracy of validation set
best_model = YOLO('/content/yolov8m-40best.pt')

# Evaluate the model on val data
results = best_model.val(data="/content/License-plate-dataset/data.yaml", imgsz=416, batch=16, conf=0.5)

#print the results
print(results)

# model accuracy on test data
# Load the trained model
best_model = YOLO('/content/yolov8m-40best.pt')

# Run validation-style evaluation on the TEST set
results = best_model.val(
    data='/content/License-plate-dataset/data.yaml',
    split='test',  # üî• this targets the test set
    imgsz=416,
    conf=0.5
)

# Print results summary
print(results)

"""**Results** -

| Dataset    | Precision | Recall  | mAP@0.5 | mAP@0.5:0.95 |
|------------|-----------|---------|---------|--------------|
| Train      | 0.981     | 0.884   | 0.940   | 0.850        |
| Validation | 0.974     | 0.887   | 0.940   | 0.842        |
| Test       | 0.970     | 0.859   | 0.924   | 0.819        |

"""



"""Training YOLO on dataset - 2"""

! pip install ultralytics

# Unzip the file
import zipfile
import os

#path to the uploaded zip file
zip_file_path = "/content/license ocr.v3i.yolov8.zip"

# unzip the file to /content folder
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
  zip_ref.extractall("/content/License-ocr-dataset")

# Check the files extracted
extracted_files = os.listdir("/content/License-ocr-dataset")
print(extracted_files)

# check the train, test and validation dataset size
train_dir = "/content/License-ocr-dataset/train/images"
val_dir = "/content/License-ocr-dataset/valid/images"
test_dir = "/content/License-ocr-dataset/test/images"

# Get the number of images in each directory
train_size = len(os.listdir(train_dir))
val_size = len(os.listdir(val_dir))
test_size = len(os.listdir(test_dir))

print(f"Train set size - {train_size}")
print(f"Validation set size - {val_size}")
print(f"Test set size - {test_size}")

# Load YAML dataset
import yaml

#path to the data.yaml file
yaml_file_path = "/content/License-ocr-dataset/data.yaml"

# Open and Load the YAML file
with open(yaml_file_path, 'r') as file:
  data_yaml = yaml.safe_load(file)

# Print the content of the YAML file
print(data_yaml)

# Extract class names from the YAML file
class_names = data_yaml.get('names', [])

# Print the list of classes
print("Classes in the Dataset:")
for class_id, class_name in enumerate(class_names):
    print(f"{class_id}: {class_name}")

from google.colab import drive
drive.mount('/content/drive')

import os

# Set a unique directory path for your current training session
save_dir = "/content/drive/MyDrive/yolo_ocr/yolov8m_license_ocr_50epoch"
os.makedirs(save_dir, exist_ok=True)

# Train the model
from ultralytics import YOLO

# Load the best weights (ensure the path is correct)
model = YOLO("/content/yolov8m-40best.pt")

# Set your training parameters including image size, batch size, half precision, and workers
model.train(
    model='yolov8m.pt',                  # Use yolov8m architecture
    data="/content/License-ocr-dataset/data.yaml",  # Path to your dataset config file
    epochs=40,                         # Train for 40 more epochs
    batch=16,                           # Batch size
    imgsz=416,                          # Image size (you can adjust this based on GPU capabilities)
    half=True,                          # Use half precision for faster training (requires CUDA)
    workers=4,                          # Number of data loading workers (adjust based on your environment)
    save=True,                          # Save weights after each epoch
    save_period=1,                      # Save every epoch
    project=save_dir,  # Path to save weights
    name='exp_yolov8m',                 # Name of experiment
    #resume=True,                        # Resume from previous model weights
)

# Load the best model weights
from ultralytics import YOLO

# Load the model
model = YOLO('/content/yolov8m-2-datasets.pt')

# print the weights
print(model)

# training metrics
train_results = model.val(data='/content/License-ocr-dataset/data.yaml',
                          split='train',
                          imgsz=416,
                          conf=0.5)

#show metrics
print(train_results)

# validation metrics
validation_results = model.val(data='/content/License-ocr-dataset/data.yaml',
                          split='val',
                          imgsz=416,
                          conf=0.5)

#show metrics
print(validation_results)

# test metrics
test_results = model.val(data='/content/License-ocr-dataset/data.yaml',
                          split='test',
                          imgsz=416,
                          conf=0.5)

#show metrics
print(test_results)

"""**Results** -
**üìä YOLOv8m OCR Model Metrics**

| Dataset      | Precision | Recall | mAP@0.5 | mAP@0.5:0.95 |
|--------------|-----------|--------|--------|--------------|
| **Training**   | 0.962     | 0.957  | 0.960  | 0.846        |
| **Validation** | 0.955     | 0.950  | 0.953  | 0.831        |
| **Testing**    | 0.956     | 0.952  | 0.957  | 0.834        |

**Import yolov8m model and Inspect**
"""

! pip install ultralytics

!apt-get install tesseract-ocr

# load model
from ultralytics import YOLO

model = YOLO('/content/yolov8m-2-datasets.pt')

# print the model weights
print(model.info)

# see model weights
print(model.names)

# Input an image
import cv2
import numpy as np

# Load your image
image_path = "/content/car-3.jpg"  # Replace with your image path
image = cv2.imread(image_path)

# Run inference on the image
results = model(image, conf=0.1)

# Check the size of the input image
print(image.shape)

import cv2
from ultralytics import YOLO
from google.colab.patches import cv2_imshow

# Load the pre-trained YOLOv8 model
model = YOLO("/content/yolov8m-2-datasets.pt")

# Load the image you want to test
image_path = "/content/car_plate.jpg"
image = cv2.imread(image_path)

# Perform inference with the YOLO model
results = model(image, conf=0.1)  # The model should be defined with weights already loaded

# Check if any predictions are made
if len(results) > 0:
    # Extract the results from the model (bounding boxes and confidence)
    predictions = results[0].boxes.xywh  # Coordinates of the detected bounding boxes
    confs = results[0].boxes.conf  # Confidence scores for the detections

    # Iterate through the predictions (bounding boxes and confidence scores)
    for i, pred in enumerate(predictions):
        # Extract bounding box coordinates (x, y, width, height)
        x1, y1, width, height = pred  # xywh format
        conf = confs[i]  # Confidence score

        # Convert from xywh (center, width, height) to x1, y1, x2, y2
        x2 = x1 + width
        y2 = y1 + height

        # Print the detected license plate details
        print(f"Detected license plate at [{x1}, {y1}, {x2}, {y2}] with confidence {conf:.2f}")

        # Crop the image around the license plate (bounding box)
        plate_image = image[int(y1):int(y2), int(x1):int(x2)]

        # Draw the bounding box on the image
        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)

    # Display the image with the bounding boxes drawn
    cv2_imshow(image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

else:
    print("No license plates detected in the image.")

model_try = YOLO('/content/yolov8m-40best.pt')

import cv2
import pytesseract
from google.colab.patches import cv2_imshow

# Load the image you want to test
image_path = "/content/car-3.jpg"
image = cv2.imread(image_path)

# Perform inference with the YOLO model
results = model_try(image, conf=0.4)  # The model should be defined with weights already loaded

# Check the content of results to understand its structure
print(results)  # Inspect the results object

# Assuming results contain predictions in the form of a list, let's process them
# Normally, we would get boxes, scores, and class indices
predictions = results[0].boxes  # This should be where the detected boxes are located

# Check if there are any detections
if len(predictions) == 0:
    print("No detections")
else:
    # Iterate through the predictions (bounding boxes and confidence scores)
    for pred in predictions:
        # Assuming pred contains the box coordinates, confidence score, and class index
        x1, y1, x2, y2 = pred.xywh[0]  # Get the bounding box coordinates (x, y, width, height)
        conf = pred.conf[0]  # Confidence score for the detection
        cls = pred.cls[0]  # Class index (for detecting a license plate, this would likely be a specific class)

        print(f"Detected license plate at [{x1}, {y1}, {x2}, {y2}] with confidence {conf:.2f}")

        # Crop the image around the license plate (bounding box)
        plate_image = image[int(y1 - y2/2):int(y1 + y2/2), int(x1 - x2/2):int(x1 + x2/2)]

        # Use Tesseract to extract the text from the cropped license plate image
        extracted_text = pytesseract.image_to_string(plate_image, config='--psm 8')  # PSM 8 for sparse text
        print(f"Extracted Text: {extracted_text}")

        # Optionally, draw the bounding box around the detected license plate
        cv2.rectangle(image, (int(x1 - x2/2), int(y1 - y2/2)), (int(x1 + x2/2), int(y1 + y2/2)), (0, 255, 0), 2)

    # Display the image with the bounding boxes drawn
    cv2_imshow(image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

import cv2
import pytesseract
from google.colab.patches import cv2_imshow

# Load the image you want to test
image_path = "/content/car-3.jpg"
image = cv2.imread(image_path)

# Perform inference with the YOLO model
results = model_try(image, conf=0.4)  # The model should be defined with weights already loaded

# Check the content of results to understand its structure
print(results)  # Inspect the results object

# Assuming results contain predictions in the form of a list, let's process them
predictions = results[0].boxes  # This should be where the detected boxes are located

# Check if there are any detections
if len(predictions) == 0:
    print("No detections")
else:
    # Iterate through the predictions (bounding boxes and confidence scores)
    for pred in predictions:
        # Get the bounding box coordinates (center_x, center_y, width, height)
        x_center, y_center, width, height = pred.xywh[0]  # YOLO returns [center_x, center_y, width, height]
        conf = pred.conf[0]  # Confidence score for the detection

        # Calculate the corner points from the center (x_center, y_center, width, height)
        x1 = int((x_center - width / 2))
        y1 = int((y_center - height / 2))
        x2 = int((x_center + width / 2))
        y2 = int((y_center + height / 2))

        print(f"Detected license plate at [{x1}, {y1}, {x2}, {y2}] with confidence {conf:.2f}")

        # Crop the image around the license plate (bounding box)
        plate_image = image[y1:y2, x1:x2]

        # Use Tesseract to extract the text from the cropped license plate image
        extracted_text = pytesseract.image_to_string(plate_image, config='--psm 8')  # PSM 8 for sparse text
        print(f"Extracted Text: {extracted_text}")

        # Optionally, draw the bounding box around the detected license plate
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

    # Display the image with the bounding boxes drawn
    cv2_imshow(image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

import cv2
import pytesseract
from google.colab.patches import cv2_imshow
import numpy as np

# Load the image you want to test
image_path = "/content/car-3.jpg"
image = cv2.imread(image_path)

# Perform inference with the YOLO model
results = model_try(image, conf=0.4)  # Assuming 'model_try' is your YOLO detection function

# Check if there are any detections
if len(results[0].boxes) == 0:
    print("No detections")
else:
    # Iterate through the predictions (bounding boxes and confidence scores)
    for pred in results[0].boxes:
        x_center, y_center, width, height = pred.xywh[0]
        conf = pred.conf[0]

        # Calculate the corner points from the center (x_center, y_center, width, height)
        x1 = int((x_center - width / 2))
        y1 = int((y_center - height / 2))
        x2 = int((x_center + width / 2))
        y2 = int((y_center + height / 2))

        print(f"Detected license plate at [{x1}, {y1}, {x2}, {y2}] with confidence {conf:.2f}")

        # Crop the image around the license plate (bounding box)
        plate_image = image[y1:y2, x1:x2]

        # Convert to grayscale
        gray_plate = cv2.cvtColor(plate_image, cv2.COLOR_BGR2GRAY)

        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced_plate = clahe.apply(gray_plate)

        # Optionally apply edge detection (Canny)
        edges = cv2.Canny(enhanced_plate, 100, 200)

        # Use adaptive thresholding for better contrast between the text and the background
        binary_plate = cv2.adaptiveThreshold(enhanced_plate, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                            cv2.THRESH_BINARY, 11, 2)

        # Apply dilation (thicker text)
        kernel = np.ones((3, 3), np.uint8)
        dilated_plate = cv2.dilate(binary_plate, kernel, iterations=1)

        # Apply Gaussian Blur for noise reduction
        dilated_plate = cv2.GaussianBlur(dilated_plate, (5, 5), 0)

        # Use Tesseract to extract the text from the processed license plate image
        extracted_text = pytesseract.image_to_string(dilated_plate, config='--psm 11')  # PSM 8 for sparse text
        print(f"Extracted Text: {extracted_text}")

        # Debugging: Using pytesseract image_to_data to visualize bounding boxes and text
        ocr_data = pytesseract.image_to_data(dilated_plate, output_type=pytesseract.Output.DICT)

        # Loop through the detected words and visualize them
        for i in range(len(ocr_data['text'])):
            if int(ocr_data['conf'][i]) > 0:  # If OCR has detected something with a positive confidence score
                (x, y, w, h) = (ocr_data['left'][i], ocr_data['top'][i], ocr_data['width'][i], ocr_data['height'][i])
                cv2.rectangle(image, (x + x1, y + y1), (x + x1 + w, y + y1 + h), (0, 255, 0), 2)
                cv2.putText(image, ocr_data['text'][i], (x + x1, y + y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        # Optionally, draw the bounding box around the detected license plate
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

    # Display the image with the bounding boxes drawn
    cv2_imshow(image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()



"""OCR - Tesseract"""

# unzip the dataset
import zipfile
import os

zip_path = "/content/license ocr.v3-3.voc.zip"
extract_path = "/content/ocr-pascal"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
  zip_ref.extractall(extract_path)

print("‚úÖ Unzipped! Files are in:", extract_path)

os.listdir(extract_path)

# count images and annotations
import os

base_path = "/content/ocr-pascal"
splits = ["train", "valid", "test"]

for split in splits:
    split_path = os.path.join(base_path, split)
    image_files = [f for f in os.listdir(split_path) if f.lower().endswith((".jpg", ".jpeg", ".png"))]
    xml_files = [f for f in os.listdir(split_path) if f.lower().endswith(".xml")]
    print(f"üìÇ {split.capitalize()} set has {len(image_files)} images and {len(xml_files)} XML annotations.")

# convert .xml into .gt.txt which tesseract is needed and replace old .xml file with .gt.txt file

import os
import xml.etree.ElementTree as ET

train_dir = "/content/ocr-pascal/test"  # Update with your path if different

# Iterate through all XML files in the train directory
for filename in os.listdir(train_dir):
    if filename.endswith(".xml"):
        xml_path = os.path.join(train_dir, filename)

        # Parse XML file
        tree = ET.parse(xml_path)
        root = tree.getroot()

        # Extract license plate text (assuming it's under the <name> tag inside <object>)
        plate_text = ""
        for obj in root.findall("object"):
            name = obj.find("name")
            if name is not None:
                plate_text = name.text

        # Save the extracted text as .gt.txt (same name as image)
        img_name = filename.replace(".xml", ".gt.txt")
        gt_txt_path = os.path.join(train_dir, img_name)

        with open(gt_txt_path, "w") as f:
            f.write(plate_text)

        # Delete the original .xml file
        os.remove(xml_path)

print("‚úÖ XMLs converted to .gt.txt and removed.")

# check the conersion
import os
import xml.etree.ElementTree as ET

train_dir = "/content/ocr-pascal/train"  # Update with your path if different

# Get the list of images
image_files = [f for f in os.listdir(train_dir) if f.lower().endswith((".jpg", ".jpeg", ".png"))]

# Initialize flags for errors
missing_files = []
mismatched_texts = []

# Iterate through images to check for matching .gt.txt and correct plate text
for img in image_files:
    gt_txt = img.replace(".jpg", ".gt.txt").replace(".jpeg", ".gt.txt").replace(".png", ".gt.txt")
    gt_txt_path = os.path.join(train_dir, gt_txt)

    # Check if the .gt.txt exists
    if not os.path.exists(gt_txt_path):
        missing_files.append(img)
        continue

    # Read plate text from .gt.txt
    with open(gt_txt_path, "r") as file:
        plate_text = file.read().strip()

    # Extract the expected plate text from the XML
    xml_path = img.replace(".jpg", ".xml").replace(".jpeg", ".xml").replace(".png", ".xml")
    xml_path = os.path.join(train_dir, xml_path)

    if os.path.exists(xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()

        # Extract the expected license plate text from XML
        expected_text = ""
        for obj in root.findall("object"):
            name = obj.find("name")
            if name is not None:
                expected_text = name.text

        # Check if the plate text in .gt.txt matches the expected text from XML
        if plate_text != expected_text:
            mismatched_texts.append((img, plate_text, expected_text))

# Print results
if missing_files:
    print("‚ùå Missing .gt.txt files for the following images:")
    print(missing_files)

if mismatched_texts:
    print("\n‚ùå Mismatched plate text found:")
    for img, gt_text, expected in mismatched_texts:
        print(f"Image: {img} -> GT Text: {gt_text} (Expected: {expected})")
else:
    print("\n‚úÖ All .gt.txt files are correctly matched with expected plate text.")

import os
import xml.etree.ElementTree as ET

# Directory paths for test and validation sets
sets = ["train", "valid", "test"]  # Add or remove any sets as needed
base_dir = "/content/ocr-pascal"  # Update this to your dataset base directory if different

# Function to check and compare .gt.txt files
def check_gt_txt(set_name):
    set_dir = os.path.join(base_dir, set_name)

    # Get the list of images in the set
    image_files = [f for f in os.listdir(set_dir) if f.lower().endswith((".jpg", ".jpeg", ".png"))]

    # Initialize flags for errors
    missing_files = []
    mismatched_texts = []

    # Iterate through images to check for matching .gt.txt and correct plate text
    for img in image_files:
        gt_txt = img.replace(".jpg", ".gt.txt").replace(".jpeg", ".gt.txt").replace(".png", ".gt.txt")
        gt_txt_path = os.path.join(set_dir, gt_txt)

        # Check if the .gt.txt exists
        if not os.path.exists(gt_txt_path):
            missing_files.append(img)
            continue

        # Read plate text from .gt.txt
        with open(gt_txt_path, "r") as file:
            plate_text = file.read().strip()

        # Extract the expected plate text from the XML
        xml_path = img.replace(".jpg", ".xml").replace(".jpeg", ".xml").replace(".png", ".xml")
        xml_path = os.path.join(set_dir, xml_path)

        if os.path.exists(xml_path):
            tree = ET.parse(xml_path)
            root = tree.getroot()

            # Extract the expected license plate text from XML
            expected_text = ""
            for obj in root.findall("object"):
                name = obj.find("name")
                if name is not None:
                    expected_text = name.text

            # Check if the plate text in .gt.txt matches the expected text from XML
            if plate_text != expected_text:
                mismatched_texts.append((img, plate_text, expected_text))

    # Print results for the current set
    if missing_files:
        print(f"‚ùå Missing .gt.txt files in {set_name.capitalize()} set for the following images:")
        print(missing_files)

    if mismatched_texts:
        print(f"\n‚ùå Mismatched plate text found in {set_name.capitalize()} set:")
        for img, gt_text, expected in mismatched_texts:
            print(f"Image: {img} -> GT Text: {gt_text} (Expected: {expected})")
    else:
        print(f"\n‚úÖ All .gt.txt files are correctly matched with expected plate text in {set_name.capitalize()} set.")

# Check all sets (train, val, test)
for set_name in sets:
    check_gt_txt(set_name)

import os

gt_folder = '/content/ocr-pascal/train'  # Path to your train folder

# List of .gt.txt files in the train folder
gt_files = [f for f in os.listdir(gt_folder) if f.endswith('.gt.txt')]

for gt_file in gt_files[:5]:  # Check the first 5 .gt.txt files for now
    gt_file_path = os.path.join(gt_folder, gt_file)
    print(f"Inspecting {gt_file_path}...")

    try:
        with open(gt_file_path, 'r') as f:
            gt_text = f.read().strip()  # Read and remove any leading/trailing spaces
            print(f"Ground truth text: {gt_text}")

            # Check if there's bounding box info or just text
            if ',' in gt_text:  # If there are commas, assume it's bounding box info
                print("This file contains bounding box coordinates.")
            else:
                print("This file contains only the ground truth text.")

            print("-" * 50)
    except Exception as e:
        print(f"Error reading {gt_file_path}: {e}")



# Install necessary dependencies for Tesseract training (if not installed yet)
!apt-get install -y tesseract-ocr
!apt-get install -y libleptonica-dev
!apt-get install -y tesseract-ocr-dev

# Install Tesseract (if not already installed)
!apt-get install tesseract-
!apt-get install tesseract-ocr-eng
!apt-get install libleptonica-dev

# Install training dependencies
!pip install pytesseract
!pip install tensorflow

from google.colab import drive
drive.mount('/content/drive')

import os

# Define the path where you want to save the weights
drive_weights_dir = '/content/drive/MyDrive/tesseract_weights'

# Create the folder if it doesn't exist
os.makedirs(drive_weights_dir, exist_ok=True)

print(f"Folder created (if it didn't exist) or already exists: {drive_weights_dir}")



!rm -rf /content/tesstrain  # Optional: Remove existing folder first

!git clone https://github.com/tesseract-ocr/tesstrain.git /content/tesstrain

!git clone https://github.com/tesseract-ocr/tesstrain /content/tesstrain

!ls /content/tesstrain

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cd /content/tesstrain
# 
# ./tesstrain.sh \
#   --fonts_dir /usr/share/fonts \
#   --lang eng \
#   --linedata_only \
#   --noextract_font_properties \
#   --langdata_dir ./langdata_lstm \
#   --tessdata_dir /usr/share/tesseract-ocr/4.00/tessdata \
#   --traineddata /usr/share/tesseract-ocr/4.00/tessdata/eng.traineddata \
#   --output_dir /content/tesseract_output \
#   --model_output /content/tesseract_output/custom \
#   --ground_truth_dir /content/ocr-text/train

import pytesseract
from PIL import Image

image_path = "/content/car_plate.jpg"
img = Image.open(image_path)

# Run OCR on the image
text = pytesseract.image_to_string(img)

print(text)



"""All steps

**Step-1: Detecting number plate with YOLOv8m model.**
"""

! pip install ultralytics

# Load the model & Detect number plates
from ultralytics import YOLO
import cv2
import matplotlib.pyplot as plt

# Load your trained YOLOv8 model
model = YOLO('/content/yolov8m-2-datasets.pt')

# Load a test image
image_path = '/content/car_plate.jpg'
image = cv2.imread(image_path)

# Run inference
results = model(image)

#plot detections
annoted_image = results[0].plot()
plt.imshow(cv2.cvtColor(annoted_image, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.title("Detected License plate")
plt.show()

from ultralytics import YOLO
import cv2
import matplotlib.pyplot as plt

# Load YOLOv8 model
model = YOLO('/content/yolov8m-40best.pt')  # change if needed

# Give all your image paths here
image_paths = [
    '/content/car-2.jpg',
    '/content/car-3.jpg',
    '/content/car_plate.jpg'
]

# Loop through each image
for img_path in image_paths:
    image = cv2.imread(img_path)
    results = model(image)

    # Draw bounding boxes
    annotated = results[0].plot()

    # Show image with detections
    plt.figure(figsize=(10, 6))
    plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))
    plt.title(f"Detections: {img_path.split('/')[-1]}")
    plt.axis('off')
    plt.show()

"""**Step-2: Crop detected plates and save them**"""

import cv2
import os

# Load YOLOv8 model
model = YOLO('/content/yolov8n-10best.pt')  # change if needed

# Give all your image paths here
image_paths = [
    '/content/car-2.jpg',
    '/content/car-3.jpg',
    '/content/car_plate.jpg'
]

# Folder to save cropped license plates
os.makedirs('/content/cropped_plates', exist_ok=True)

for i, img_path in enumerate(image_paths):
    image = cv2.imread(img_path)
    results = model(image)

    # Get detected bounding boxes
    boxes = results[0].boxes.xyxy.cpu().numpy()

    for j, box in enumerate(boxes):
        x1, y1, x2, y2 = map(int, box)

        # Crop the license plate
        cropped = image[y1:y2, x1:x2]

        # Save the cropped image
        crop_path = f"/content/cropped_plates/plate_{i}_{j}.jpg"
        cv2.imwrite(crop_path, cropped)
        print(f"Cropped and saved: {crop_path}")

# Display the cropped images
import matplotlib.pyplot as plt

# Folder containing cropped plates
crop_folder = '/content/cropped_plates'

# Get all cropped image paths
cropped_images = sorted([os.path.join(crop_folder, f) for f in os.listdir(crop_folder) if f.endswith('.jpg')])

# Plot each image
plt.figure(figsize=(12, 4))
for idx, img_path in enumerate(cropped_images):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.subplot(1, len(cropped_images), idx + 1)
    plt.imshow(img)
    plt.title(f'Plate {idx}')
    plt.axis('off')

plt.tight_layout()
plt.show()

"""**Step 3: Perform OCR on Cropped Plates**

Tesseract
"""

!apt-get install tesseract-ocr
!pip install pytesseract

import pytesseract
from PIL import Image
import os

# Path to the folder containing cropped images
cropped_folder = '/content/cropped_plates/'

# Function to apply OCR on each cropped image
def perform_ocr_on_cropped_images(cropped_folder):
    # List all files in the folder
    cropped_images = [f for f in os.listdir(cropped_folder) if f.endswith('.jpg')]

    # Loop through each image file
    for image_name in cropped_images:
        image_path = os.path.join(cropped_folder, image_name)

        # Open image using PIL
        img = Image.open(image_path)

        # Apply OCR on the image
        extracted_text = pytesseract.image_to_string(img)

        # Print the extracted text
        print(f"Text from {image_name}: {extracted_text}\n")

# Perform OCR on the cropped images
perform_ocr_on_cropped_images(cropped_folder)

import pytesseract
from PIL import Image
import cv2
import os

# Path to the folder containing cropped images
cropped_folder = '/content/cropped_plates/'

# Function to preprocess image and apply OCR
def preprocess_and_ocr(image_path):
    # Read the image using OpenCV
    img = cv2.imread(image_path)

    # Convert the image to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Apply adaptive thresholding to get a binary image
    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                   cv2.THRESH_BINARY, 11, 2)

    # Optionally, remove noise using a median blur
    thresh = cv2.medianBlur(thresh, 3)

    # Dilate the text to make it thicker
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    dilated = cv2.dilate(thresh, kernel, iterations=1)

    # Resize the image (optional)
    height, width = dilated.shape
    new_width = int(width * 1.5)  # Increase width by 1.5 times
    new_height = int(height * 1.5)  # Increase height by 1.5 times
    resized = cv2.resize(dilated, (new_width, new_height))

    # Convert the processed image to PIL format for Tesseract
    pil_img = Image.fromarray(resized)

    # Run OCR on the processed image with Tesseract config
    custom_config = r'--oem 1 --psm 8'  # Try different psm modes (6 is good for text lines)
    text = pytesseract.image_to_string(pil_img, config=custom_config)

    return text

# Process each cropped image and perform OCR
for i, filename in enumerate(os.listdir(cropped_folder)):
    if filename.endswith(".jpg"):
        image_path = os.path.join(cropped_folder, filename)
        extracted_text = preprocess_and_ocr(image_path)
        print(f"Text from {filename}: {extracted_text}\n")

"""Easy OCR"""

!pip install easyocr

import easyocr
import os
from PIL import Image

# Create an EasyOCR reader instance (English language)
reader = easyocr.Reader(['en'])

# Path to cropped plate images folder
cropped_folder = '/content/cropped_plates/'

# Get the list of cropped images
cropped_images = [f for f in os.listdir(cropped_folder) if f.endswith('.jpg')]

# Iterate through each cropped image and perform OCR
for image_name in cropped_images:
    image_path = os.path.join(cropped_folder, image_name)

    # Perform OCR on the image
    result = reader.readtext(image_path)

    # Print OCR results
    print(f"Text detected in {image_name}:")
    for detection in result:
        print(f"Detected Text: {detection[1]}")
    print("-" * 50)

"""**Step-3 - Backend**"""

import easyocr
import os

# Create an EasyOCR reader instance (English language)
reader = easyocr.Reader(['en'])

# Path to cropped plate images folder
cropped_folder = '/content/cropped_plates/'

# Get the list of cropped images
cropped_images = [f for f in os.listdir(cropped_folder) if f.endswith('.jpg')]

# Extract text from each image and save it
extracted_texts = {}

for image_name in cropped_images:
    image_path = os.path.join(cropped_folder, image_name)

    # Perform OCR on the image
    result = reader.readtext(image_path)

    # Store the extracted text from OCR
    extracted_text = ' '.join([detection[1] for detection in result])

    # Save extracted text for later use
    extracted_texts[image_name] = extracted_text

    # Print OCR results
    print(f"Text detected in {image_name}: {extracted_text}")
    print("-" * 50)

# Optionally save the extracted text to a file or return it for future use
# Example: save to a text file
with open("extracted_texts.txt", "w") as file:
    for image_name, text in extracted_texts.items():
        file.write(f"{image_name}: {text}\n")

# You can use extracted_texts dictionary or saved file for Step 4

# Example database of known license plates
known_plates = ['EW 588', 'GD KARMA', 'SL 593 LM']

# Function to compare the extracted text with known plates
def compare_text(extracted_text):
    for plate in known_plates:
        if plate in extracted_text:
            return True, plate
    return False, None

# Here, we load the extracted texts from Step 3 (this could be from a file or variable)
# For now, we will assume the `extracted_texts` dictionary from Step 3 is available

# Compare extracted texts with known plates
for image_name, extracted_text in extracted_texts.items():
    print(f"Comparing text from {image_name}: {extracted_text}")

    match_found, matched_plate = compare_text(extracted_text)

    if match_found:
        print(f"Match Found: {matched_plate}")
        # Trigger your alert system or pop-up here
    else:
        print("No match found.")

    print("-" * 50)



"""**Step-4 - Alert System**"""

from IPython.display import display, HTML

# Function to display an alert message within the notebook
def show_alert(plate):
    display(HTML(f"<h3 style='color:green;'>Match Found: {plate}</h3>"))

# Compare extracted texts with known plates and trigger alert if match found
for image_name, extracted_text in extracted_texts.items():
    print(f"Comparing text from {image_name}: {extracted_text}")

    match_found, matched_plate = compare_text(extracted_text)

    if match_found:
        print(f"Match Found: {matched_plate}")
        show_alert(matched_plate)  # Show alert in the notebook
    else:
        print("No match found.")

    print("-" * 50)



"""Using SQL-Lite Database"""

import sqlite3

# Create a connection to a database (or it will create one if it doesn't exist)
conn = sqlite3.connect('license_plates.db')

# Create a cursor object to interact with the database
cursor = conn.cursor()

# Create a table to store license plates
cursor.execute('''
CREATE TABLE IF NOT EXISTS plates (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    plate_number TEXT NOT NULL
)
''')

# Commit the changes
conn.commit()

# Insert license plates into the database

import random

# List of known license plates (including our original plates)
known_plates = ['EW 588', 'GD KARMA', 'SL 593 LM']

# Generate 50 random plates (for example, using random characters and numbers)
random_plates = []
for _ in range(50):
    random_plate = ''.join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', k=7))  # Random 7-character plates
    random_plates.append(random_plate)

# Combine original plates with random plates
all_plates = known_plates + random_plates

# Insert all plates into the database
for plate in all_plates:
    cursor.execute('''
    INSERT INTO plates (plate_number)
    VALUES (?)
    ''', (plate,))

# Commit the changes
conn.commit()

# Function to compare the extracted text with known plates in the database
def compare_with_database(extracted_text):
    cursor.execute('SELECT * FROM plates')  # Fetching all rows
    rows = cursor.fetchall()  # Fetch all plates from the database

    for row in rows:
        db_plate = str(row[1])  # Assuming the plate number is in the second column (index 1), change this if needed

        # Ensure both plate number and extracted text are treated as strings
        if db_plate.strip() in extracted_text.strip():  # Strip any leading/trailing spaces
            return True, db_plate  # Return True and the matching plate number

    return False, None  # If no match found, return False and None

# Compare each extracted text with the database
for text in extracted_texts:
    match_found, matched_plate = compare_with_database(text)

    if match_found:
        print(f"Text: {text} -> Match Found: {matched_plate}")
    else:
        print(f"Text: {text} -> No match found.")



"""Lost vehicle.."""

import sqlite3
import random

# Connect to SQLite database
conn = sqlite3.connect('vehicle_tracking.db')
cursor = conn.cursor()

# Create table for lost vehicles
cursor.execute('''
    CREATE TABLE IF NOT EXISTS lost_vehicles (
        plate TEXT PRIMARY KEY,
        make TEXT,
        model TEXT,
        color TEXT,
        last_seen TEXT
    )
''')

# Generate 20 random license plates
def generate_random_plate():
    letters = random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ', k=3)
    digits = random.choices('0123456789', k=4)
    return ''.join(letters) + ' ' + ''.join(digits)

# Insert 20 random plates into the database
def insert_random_plates():
    for _ in range(20):
        plate = generate_random_plate()
        make = random.choice(['Toyota', 'Honda', 'Ford', 'Chevrolet', 'BMW'])
        model = random.choice(['Camry', 'Civic', 'Mustang', 'Malibu', 'X5'])
        color = random.choice(['Red', 'Blue', 'Green', 'Black', 'White'])
        last_seen = "2025-04-09"

        cursor.execute('''
            INSERT OR REPLACE INTO lost_vehicles (plate, make, model, color, last_seen)
            VALUES (?, ?, ?, ?, ?)
        ''', (plate, make, model, color, last_seen))

    conn.commit()

# Call function to insert random plates
insert_random_plates()

# Check the data
cursor.execute("SELECT * FROM lost_vehicles")
rows = cursor.fetchall()
for row in rows:
    print(row)

conn.close()

!pip install Flask

from flask import Flask, render_template, request, redirect, url_for
import sqlite3

app = Flask(__name__)

# Connect to the SQLite database
def get_db_connection():
    conn = sqlite3.connect('vehicle_tracking.db')
    conn.row_factory = sqlite3.Row
    return conn

# Home route to display all vehicles
@app.route('/')
def index():
    conn = get_db_connection()
    vehicles = conn.execute('SELECT * FROM lost_vehicles').fetchall()
    conn.close()
    return render_template('index.html', vehicles=vehicles)

# Route to handle the form submission
@app.route('/add_lost_vehicle', methods=['GET', 'POST'])
def add_lost_vehicle():
    if request.method == 'POST':
        plate = request.form['plate']
        make = request.form['make']
        model = request.form['model']
        color = request.form['color']
        last_seen = request.form['last_seen']

        # Insert the new vehicle into the database
        conn = get_db_connection()
        conn.execute('''
            INSERT INTO lost_vehicles (plate, make, model, color, last_seen)
            VALUES (?, ?, ?, ?, ?)
        ''', (plate, make, model, color, last_seen))
        conn.commit()
        conn.close()
        return redirect(url_for('index'))

    return render_template('add_lost_vehicle.html')

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)

from flask import Flask, render_template, request

app = Flask(__name__)

@app.route('/')
def index():
    return render_template('index.html')  # Make sure you have an index.html in the templates folder

@app.route('/submit', methods=['POST'])
def submit_plate():
    if request.method == 'POST':
        plate = request.form['plate']
        # Save the plate to the database or perform other actions
        print(f"Plate added: {plate}")
        return f"Plate {plate} has been added!"

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)

